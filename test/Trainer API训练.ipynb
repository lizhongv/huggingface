{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e8b2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 https://zhuanlan.zhihu.com/p/390823624\n",
    "# 参考 https://blog.csdn.net/weixin_45397053/article/details/120478054\n",
    "# 参考 https://segmentfault.com/a/1190000041524159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165fe92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a63ecfa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/lizhong/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b38b77286964df4a68f955b3caf8210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother , whom he called \" ...</td>\n",
       "      <td>Referring to him as only \" the witness \" , Amr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick 's before selling the c...</td>\n",
       "      <td>Yucaipa bought Dominick 's in 1995 for $ 693 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10 , the ship 's owners had published ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT , Tab shares were up 19 cents ...</td>\n",
       "      <td>Tab shares jumped 20 cents , or 4.6 % , to set...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $ 2.11 , or about 11 percent , ...</td>\n",
       "      <td>PG &amp; E Corp. shares jumped $ 1.63 or 8 percent...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>\" At this point , Mr. Brando announced : ' Som...</td>\n",
       "      <td>Brando said that \" somebody ought to put a bul...</td>\n",
       "      <td>1</td>\n",
       "      <td>4071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>Martin , 58 , will be freed today after servin...</td>\n",
       "      <td>Martin served two thirds of a five-year senten...</td>\n",
       "      <td>0</td>\n",
       "      <td>4072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>\" We have concluded that the outlook for price...</td>\n",
       "      <td>In a statement , the ECB said the outlook for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>The notification was first reported Friday by ...</td>\n",
       "      <td>MSNBC.com first reported the CIA request on Fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>4074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>The 30-year bond US30YT = RR rose 22 / 32 for ...</td>\n",
       "      <td>The 30-year bond US30YT = RR grew 1-3 / 32 for...</td>\n",
       "      <td>0</td>\n",
       "      <td>4075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3668 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence1  \\\n",
       "0     Amrozi accused his brother , whom he called \" ...   \n",
       "1     Yucaipa owned Dominick 's before selling the c...   \n",
       "2     They had published an advertisement on the Int...   \n",
       "3     Around 0335 GMT , Tab shares were up 19 cents ...   \n",
       "4     The stock rose $ 2.11 , or about 11 percent , ...   \n",
       "...                                                 ...   \n",
       "3663  \" At this point , Mr. Brando announced : ' Som...   \n",
       "3664  Martin , 58 , will be freed today after servin...   \n",
       "3665  \" We have concluded that the outlook for price...   \n",
       "3666  The notification was first reported Friday by ...   \n",
       "3667  The 30-year bond US30YT = RR rose 22 / 32 for ...   \n",
       "\n",
       "                                              sentence2  label   idx  \n",
       "0     Referring to him as only \" the witness \" , Amr...      1     0  \n",
       "1     Yucaipa bought Dominick 's in 1995 for $ 693 m...      0     1  \n",
       "2     On June 10 , the ship 's owners had published ...      1     2  \n",
       "3     Tab shares jumped 20 cents , or 4.6 % , to set...      0     3  \n",
       "4     PG & E Corp. shares jumped $ 1.63 or 8 percent...      1     4  \n",
       "...                                                 ...    ...   ...  \n",
       "3663  Brando said that \" somebody ought to put a bul...      1  4071  \n",
       "3664  Martin served two thirds of a five-year senten...      0  4072  \n",
       "3665  In a statement , the ECB said the outlook for ...      1  4073  \n",
       "3666  MSNBC.com first reported the CIA request on Fr...      1  4074  \n",
       "3667  The 30-year bond US30YT = RR grew 1-3 / 32 for...      0  4075  \n",
       "\n",
       "[3668 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据集\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")  \n",
    "raw_datasets  # DatasetDict   [train/validation/test]\n",
    "# 文本相似度   label = 1: 代表句子是同义词 - label = 0: 代表句子不是同义词\n",
    "\n",
    "train_dataset = raw_datasets['train']  \n",
    "# type(train_dataset)\n",
    "# dir(train_dataset)\n",
    "# train_dataset  # Dataset   features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "# train_dataset.features\n",
    "# train_dataset.column_names\n",
    "# train_dataset[0]\n",
    "\n",
    "# train = pd.DataFrame(train_dataset)\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e2ef01",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 加载分词编码工具\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fase=True)\n",
    "\n",
    "# 加载模型（model head：ForSequenceClassification）\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77239d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例子\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\",\n",
    "    \"This is the third sentence.\",\n",
    "    \"To access an actual element, you need to select a split first, then give an index.\"\n",
    "]\n",
    "\n",
    "# 分词编码\n",
    "batch = tokenizer(sequences, \n",
    "                  truncation=True,\n",
    "                  padding=\"max_length\",  \n",
    "                  max_length=20,\n",
    "                  return_tensors=\"pt\")\n",
    "batch\n",
    "# type(batch)  # transformers.tokenization_utils_base.BatchEncoding\n",
    "# dir(batch)\n",
    "# batch.keys()  # dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "# for k,v in batch.items():\n",
    "#     print(k, ':', v)\n",
    "    \n",
    "# print(batch['input_ids'])\n",
    "# print(tokenizer.decode(batch['input_ids'][0]))\n",
    "# print(tokenizer.decode(batch['input_ids'][1]))\n",
    "# print(tokenizer.decode(batch['input_ids'][2]))\n",
    "# print(tokenizer.decode(batch['input_ids'][3]))\n",
    "\n",
    "# print(batch['input_ids'].tolist())\n",
    "# print(tokenizer.decode(batch['input_ids'].tolist()[0]))\n",
    "# print(tokenizer.decode(batch['input_ids'].tolist()[1]))\n",
    "# print(tokenizer.decode(batch['input_ids'].tolist()[2]))\n",
    "# print(tokenizer.decode(batch['input_ids'].tolist()[3]))\n",
    "# help(tokenizer.decode)\n",
    "\n",
    "batch['labels'] = torch.tensor([1, 1, 0, 1])\n",
    "optimizer = AdamW(model.parameters())\n",
    "# 序列解包 list(range(*[3,6]))  \n",
    "# 字典解包 {'x':1, **{'y':2, 'z':3}} \n",
    "outputs = model(**batch)  \n",
    "outputs.logits.shape\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predictions\n",
    "\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# tokenized_text = tokenizer(\"This is the first sentence.\", \"This is the second one.\",\"This is the third sentence.\")  # 第三个句子不显示\n",
    "# tokenized_text, tokenizer.decode(tokenized_text['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ea8c96",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 训练集预处理\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 将句子对列表传给tokenizer，对整个数据集进行分词处理\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1.这种方式不能保证结果为dataset形式，且需要将整个dataset加载到RAM中\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(\n\u001b[0;32m      6\u001b[0m     raw_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      7\u001b[0m     raw_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      8\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mtype\u001b[39m(tokenized_dataset)  \u001b[38;5;66;03m# transformers.tokenization_utils_base.BatchEncoding\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mdir\u001b[39m(tokenized_dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 训练集预处理\n",
    "# 将句子对列表传给tokenizer，对整个数据集进行分词处理\n",
    "\n",
    "# 1.这种方式不能保证结果为dataset形式，且需要将整个dataset加载到RAM中\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets['train']['sentence1'],\n",
    "    raw_datasets['train']['sentence2'],\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt')\n",
    "\n",
    "type(tokenized_dataset)  # transformers.tokenization_utils_base.BatchEncoding\n",
    "dir(tokenized_dataset)\n",
    "tokenized_dataset.keys() # dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"input_ids\"].shape  # torch.Size([3668, 103])\n",
    "\n",
    "del tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0c6e0d8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW\t AutoModelForSequenceClassification\t AutoTokenizer\t DataCollatorWithPadding\t Dataset\t Trainer\t TrainingArguments\t batch\t checkpoint\t \n",
      "collate_fn\t compute_metrics\t data_collator\t load_dataset\t loss\t model\t optimizer\t outputs\t pd\t \n",
      "predictions\t raw_datasets\t sequences\t tokenize_function\t tokenizer\t torch\t train_args\t trainer\t training_args\t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "080dd99a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f699da3ab742d98e61832b2de11304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\lizhong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-28520c057cc163de.arrow\n",
      "Loading cached processed dataset at C:\\Users\\lizhong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-95d8f6fd48c94828.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. map方法保持dataset格式，这里没有使用padding到模型最大长度(效率低)，在构造batch时再进行padding到batch最大长度\n",
    "def tokenize_function(example: dict):\n",
    "    return tokenizer(example[\"sentence1\"], \n",
    "                     example[\"sentence2\"], \n",
    "                     truncation=True, \n",
    "                     padding=True,                 \n",
    "                     return_tensors=\"pt\"\n",
    "                    )\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)  # batch=True可以同时批处理\n",
    "tokenized_datasets  # DatasetDict [train/validation/test]\n",
    "\n",
    "# train_dataset = tokenized_datasets['train']\n",
    "# dir(train_dataset)\n",
    "# type(train_dataset)  # datasets.arrow_dataset.Dataset\n",
    "# train_dataset.features\n",
    "# train_dataset.column_names\n",
    "\n",
    "# for key, value in tokenized_datasets['train'][0].items():\n",
    "#     print(key, ':', value)\n",
    "# tokenizer.decode(tokenized_datasets['train'][0]['input_ids'])\n",
    "\n",
    "# train = pd.DataFrame(train_dataset)\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3e51f654",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [1, 0, 1, 0, 1, 1, 0, 1],\n",
       " 'input_ids': [[101,\n",
       "   2572,\n",
       "   3217,\n",
       "   5831,\n",
       "   5496,\n",
       "   2010,\n",
       "   2567,\n",
       "   1010,\n",
       "   3183,\n",
       "   2002,\n",
       "   2170,\n",
       "   1000,\n",
       "   1996,\n",
       "   7409,\n",
       "   1000,\n",
       "   1010,\n",
       "   1997,\n",
       "   9969,\n",
       "   4487,\n",
       "   23809,\n",
       "   3436,\n",
       "   2010,\n",
       "   3350,\n",
       "   1012,\n",
       "   102,\n",
       "   7727,\n",
       "   2000,\n",
       "   2032,\n",
       "   2004,\n",
       "   2069,\n",
       "   1000,\n",
       "   1996,\n",
       "   7409,\n",
       "   1000,\n",
       "   1010,\n",
       "   2572,\n",
       "   3217,\n",
       "   5831,\n",
       "   5496,\n",
       "   2010,\n",
       "   2567,\n",
       "   1997,\n",
       "   9969,\n",
       "   4487,\n",
       "   23809,\n",
       "   3436,\n",
       "   2010,\n",
       "   3350,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   9805,\n",
       "   3540,\n",
       "   11514,\n",
       "   2050,\n",
       "   3079,\n",
       "   11282,\n",
       "   2243,\n",
       "   1005,\n",
       "   1055,\n",
       "   2077,\n",
       "   4855,\n",
       "   1996,\n",
       "   4677,\n",
       "   2000,\n",
       "   3647,\n",
       "   4576,\n",
       "   1999,\n",
       "   2687,\n",
       "   2005,\n",
       "   1002,\n",
       "   1016,\n",
       "   1012,\n",
       "   1019,\n",
       "   4551,\n",
       "   1012,\n",
       "   102,\n",
       "   9805,\n",
       "   3540,\n",
       "   11514,\n",
       "   2050,\n",
       "   4149,\n",
       "   11282,\n",
       "   2243,\n",
       "   1005,\n",
       "   1055,\n",
       "   1999,\n",
       "   2786,\n",
       "   2005,\n",
       "   1002,\n",
       "   6353,\n",
       "   2509,\n",
       "   2454,\n",
       "   1998,\n",
       "   2853,\n",
       "   2009,\n",
       "   2000,\n",
       "   3647,\n",
       "   4576,\n",
       "   2005,\n",
       "   1002,\n",
       "   1015,\n",
       "   1012,\n",
       "   1022,\n",
       "   4551,\n",
       "   1999,\n",
       "   2687,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   2027,\n",
       "   2018,\n",
       "   2405,\n",
       "   2019,\n",
       "   15147,\n",
       "   2006,\n",
       "   1996,\n",
       "   4274,\n",
       "   2006,\n",
       "   2238,\n",
       "   2184,\n",
       "   1010,\n",
       "   5378,\n",
       "   1996,\n",
       "   6636,\n",
       "   2005,\n",
       "   5096,\n",
       "   1010,\n",
       "   2002,\n",
       "   2794,\n",
       "   1012,\n",
       "   102,\n",
       "   2006,\n",
       "   2238,\n",
       "   2184,\n",
       "   1010,\n",
       "   1996,\n",
       "   2911,\n",
       "   1005,\n",
       "   1055,\n",
       "   5608,\n",
       "   2018,\n",
       "   2405,\n",
       "   2019,\n",
       "   15147,\n",
       "   2006,\n",
       "   1996,\n",
       "   4274,\n",
       "   1010,\n",
       "   5378,\n",
       "   1996,\n",
       "   14792,\n",
       "   2005,\n",
       "   5096,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   2105,\n",
       "   6021,\n",
       "   19481,\n",
       "   13938,\n",
       "   2102,\n",
       "   1010,\n",
       "   21628,\n",
       "   6661,\n",
       "   2020,\n",
       "   2039,\n",
       "   2539,\n",
       "   16653,\n",
       "   1010,\n",
       "   2030,\n",
       "   1018,\n",
       "   1012,\n",
       "   1018,\n",
       "   1003,\n",
       "   1010,\n",
       "   2012,\n",
       "   1037,\n",
       "   1002,\n",
       "   1018,\n",
       "   1012,\n",
       "   5179,\n",
       "   1010,\n",
       "   2383,\n",
       "   3041,\n",
       "   2275,\n",
       "   1037,\n",
       "   2501,\n",
       "   2152,\n",
       "   1997,\n",
       "   1037,\n",
       "   1002,\n",
       "   1018,\n",
       "   1012,\n",
       "   5401,\n",
       "   1012,\n",
       "   102,\n",
       "   21628,\n",
       "   6661,\n",
       "   5598,\n",
       "   2322,\n",
       "   16653,\n",
       "   1010,\n",
       "   2030,\n",
       "   1018,\n",
       "   1012,\n",
       "   1020,\n",
       "   1003,\n",
       "   1010,\n",
       "   2000,\n",
       "   2275,\n",
       "   1037,\n",
       "   2501,\n",
       "   5494,\n",
       "   2152,\n",
       "   2012,\n",
       "   1037,\n",
       "   1002,\n",
       "   1018,\n",
       "   1012,\n",
       "   5401,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   1996,\n",
       "   4518,\n",
       "   3123,\n",
       "   1002,\n",
       "   1016,\n",
       "   1012,\n",
       "   2340,\n",
       "   1010,\n",
       "   2030,\n",
       "   2055,\n",
       "   2340,\n",
       "   3867,\n",
       "   1010,\n",
       "   2000,\n",
       "   2485,\n",
       "   5958,\n",
       "   2012,\n",
       "   1002,\n",
       "   2538,\n",
       "   1012,\n",
       "   4868,\n",
       "   2006,\n",
       "   1996,\n",
       "   2047,\n",
       "   2259,\n",
       "   4518,\n",
       "   3863,\n",
       "   1012,\n",
       "   102,\n",
       "   18720,\n",
       "   1004,\n",
       "   1041,\n",
       "   13058,\n",
       "   1012,\n",
       "   6661,\n",
       "   5598,\n",
       "   1002,\n",
       "   1015,\n",
       "   1012,\n",
       "   6191,\n",
       "   2030,\n",
       "   1022,\n",
       "   3867,\n",
       "   2000,\n",
       "   1002,\n",
       "   2538,\n",
       "   1012,\n",
       "   6021,\n",
       "   2006,\n",
       "   1996,\n",
       "   2047,\n",
       "   2259,\n",
       "   4518,\n",
       "   3863,\n",
       "   2006,\n",
       "   5958,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   6599,\n",
       "   1999,\n",
       "   1996,\n",
       "   2034,\n",
       "   4284,\n",
       "   1997,\n",
       "   1996,\n",
       "   2095,\n",
       "   3333,\n",
       "   2321,\n",
       "   3867,\n",
       "   2013,\n",
       "   1996,\n",
       "   2168,\n",
       "   2558,\n",
       "   1037,\n",
       "   2095,\n",
       "   3041,\n",
       "   1012,\n",
       "   102,\n",
       "   2007,\n",
       "   1996,\n",
       "   9446,\n",
       "   5689,\n",
       "   2058,\n",
       "   5954,\n",
       "   1005,\n",
       "   1055,\n",
       "   2194,\n",
       "   1010,\n",
       "   6599,\n",
       "   1996,\n",
       "   2034,\n",
       "   4284,\n",
       "   1997,\n",
       "   1996,\n",
       "   2095,\n",
       "   3333,\n",
       "   2321,\n",
       "   3867,\n",
       "   2013,\n",
       "   1996,\n",
       "   2168,\n",
       "   2558,\n",
       "   1037,\n",
       "   2095,\n",
       "   3041,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   1996,\n",
       "   17235,\n",
       "   2850,\n",
       "   4160,\n",
       "   2018,\n",
       "   1037,\n",
       "   4882,\n",
       "   5114,\n",
       "   1997,\n",
       "   2459,\n",
       "   1012,\n",
       "   2676,\n",
       "   1010,\n",
       "   2030,\n",
       "   1015,\n",
       "   1012,\n",
       "   1016,\n",
       "   3867,\n",
       "   1010,\n",
       "   5494,\n",
       "   2012,\n",
       "   1015,\n",
       "   1010,\n",
       "   19611,\n",
       "   1012,\n",
       "   2321,\n",
       "   2006,\n",
       "   5958,\n",
       "   1012,\n",
       "   102,\n",
       "   1996,\n",
       "   6627,\n",
       "   1011,\n",
       "   17958,\n",
       "   17235,\n",
       "   2850,\n",
       "   4160,\n",
       "   12490,\n",
       "   1012,\n",
       "   11814,\n",
       "   2594,\n",
       "   24356,\n",
       "   2382,\n",
       "   1012,\n",
       "   4805,\n",
       "   2685,\n",
       "   1010,\n",
       "   2030,\n",
       "   1016,\n",
       "   1012,\n",
       "   5840,\n",
       "   3867,\n",
       "   1010,\n",
       "   2000,\n",
       "   1015,\n",
       "   1010,\n",
       "   19611,\n",
       "   1012,\n",
       "   2321,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   1996,\n",
       "   4966,\n",
       "   1011,\n",
       "   10507,\n",
       "   2050,\n",
       "   2059,\n",
       "   12068,\n",
       "   2000,\n",
       "   1996,\n",
       "   2110,\n",
       "   4259,\n",
       "   2457,\n",
       "   1012,\n",
       "   102,\n",
       "   1996,\n",
       "   4966,\n",
       "   10507,\n",
       "   2050,\n",
       "   12068,\n",
       "   2008,\n",
       "   3247,\n",
       "   2000,\n",
       "   1996,\n",
       "   1057,\n",
       "   1012,\n",
       "   1055,\n",
       "   1012,\n",
       "   4259,\n",
       "   2457,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'token_type_ids': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collate将一个batch数据进行聚合，将数据样本(list,tuples,dictionary)转化为pytorch tensor，进行拼接\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 例子\n",
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "type(samples)  # dict\n",
    "samples = { k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"] }\n",
    "pd.DataFrame(samples)\n",
    "samples\n",
    "# [len(x) for x in samples[\"input_ids\"]]\n",
    "\n",
    "# batch = data_collator(samples) \n",
    "# type(batch)  # transformers.tokenization_utils_base.BatchEncoding\n",
    "# {k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "670ee4a3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW\t AutoModelForSequenceClassification\t AutoTokenizer\t DataCollatorWithPadding\t Dataset\t batch\t checkpoint\t data_collator\t k\t \n",
      "load_dataset\t loss\t model\t optimizer\t raw_datasets\t samples\t sequences\t tokenize_function\t tokenized_dataset\t \n",
      "tokenized_datasets\t tokenizer\t torch\t train_dataset\t v\t \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "capture": "ExecutionMagics",
        "cmd": "Other",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "js": "DisplayMagics",
        "latex": "DisplayMagics",
        "markdown": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autoawait": "AsyncMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "cls": "KernelMagics",
        "colors": "BasicMagics",
        "conda": "PackagingMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "copy": "Other",
        "ddir": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "echo": "Other",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "matplotlib": "PylabMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "pip": "PackagingMagics",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "ren": "Other",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cd  %clear  %cls  %colors  %conda  %config  %connect_info  %copy  %ddir  %debug  %dhist  %dirs  %doctest_mode  %echo  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %macro  %magic  %matplotlib  %mkdir  %more  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %ren  %rep  %rerun  %reset  %reset_selective  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%capture  %%cmd  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%who\n",
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256122dd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/lizhong/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dec65d4ce0f44e9a6373b5291a3563f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\lizhong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-0934c045dd1afe10.arrow\n",
      "Loading cached processed dataset at C:\\Users\\lizhong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-0ca0a34349921406.arrow\n",
      "Loading cached processed dataset at C:\\Users\\lizhong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-c3b222f285c62abb.arrow\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\software\\anaconda3\\envs\\py3.9\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1377\n",
      "  Number of trainable parameters = 109483778\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 03:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.545500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.347300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkpoint\\checkpoint-500\n",
      "Configuration saved in checkpoint\\checkpoint-500\\config.json\n",
      "Model weights saved in checkpoint\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in checkpoint\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to checkpoint\\checkpoint-1000\n",
      "Configuration saved in checkpoint\\checkpoint-1000\\config.json\n",
      "Model weights saved in checkpoint\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in checkpoint\\checkpoint-1000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.37798399083754597, metrics={'train_runtime': 216.3198, 'train_samples_per_second': 50.869, 'train_steps_per_second': 6.366, 'total_flos': 405470580750720.0, 'train_loss': 0.37798399083754597, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.数据预处理\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 加载数据\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# 加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# 数据处理\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], \n",
    "                     example[\"sentence2\"], \n",
    "                     truncation=True)  # return_tensors='pt'\n",
    "\n",
    "# 分词编码\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # 动态padding，转换tensor\n",
    "\n",
    "# 设置训练的参数\n",
    "training_args = TrainingArguments(\"checkpoint\")  # 模型保存的路径\n",
    "\n",
    "# 加载模型（model head）\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# Trainer 进行精调\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,  # collator\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "    \n",
    "# 训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9b2fc5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lizhong\\AppData\\Local\\Temp\\ipykernel_29844\\2607169778.py:16: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', 'mrpc')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8455882352941176, 'f1': 0.8934010152284263}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions  # namedtuple\n",
    "\n",
    "# predictions.predictions.shape  # logits 并没有经过softmax \n",
    "# predictions.label_ids.shape\n",
    "# predictions.metrics\n",
    "\n",
    "import numpy as np\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "preds\n",
    "\n",
    "# 构造评价指标\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "metric.compute(predictions=preds,\n",
    "              references=predictions.label_ids)\n",
    "\n",
    "# # 新版本\n",
    "# import evaluate\n",
    "# accuracy = evaluate.load(\"accuracy\")\n",
    "# accuracy.compute(references=predictions.label_ids,\n",
    "#                  predictions=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ea22d1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/lizhong/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97df6c30b575450bb1798d33e76ba2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\lizhong/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at C:\\Users\\lizhong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-0934c045dd1afe10.arrow\n",
      "Loading cached processed dataset at C:\\Users\\lizhong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-0ca0a34349921406.arrow\n",
      "Loading cached processed dataset at C:\\Users\\lizhong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-c3b222f285c62abb.arrow\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\software\\anaconda3\\envs\\py3.9\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1377\n",
      "  Number of trainable parameters = 109483778\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 03:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.336000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer\\checkpoint-500\n",
      "Configuration saved in test-trainer\\checkpoint-500\\config.json\n",
      "Model weights saved in test-trainer\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in test-trainer\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer\\checkpoint-1000\n",
      "Configuration saved in test-trainer\\checkpoint-1000\\config.json\n",
      "Model weights saved in test-trainer\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in test-trainer\\checkpoint-1000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3739865648633947, metrics={'train_runtime': 209.2047, 'train_samples_per_second': 52.599, 'train_steps_per_second': 6.582, 'total_flos': 405470580750720.0, 'train_loss': 0.3739865648633947, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.数据预处理\n",
    "import torch\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 加载数据\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# 加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                          num_labels=2)\n",
    "\n",
    "# 数据处理\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], \n",
    "                     example[\"sentence2\"], \n",
    "                     truncation=True)  # return_tensors='pt'\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # 动态padding，\n",
    "\n",
    "# 包装成一个函数\n",
    "def compute_metrics(eval_preds: tuple):\n",
    "    metric = load_metric('glue', 'mrpc')\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds,\n",
    "                         references=labels)\n",
    "\n",
    "# from transformers.trainer_utils import EvalPrediction\n",
    "# # 模拟测试输出\n",
    "# eval_pred = EvalPrediction(predictions=np.array([[0, 1], [2, 3], [4, 5], [6, 7]]),\n",
    "#                            label_ids=np.array([1, 1, 1, 1]))\n",
    "\n",
    "# compute_metrics(eval_pred)\n",
    "\n",
    "\n",
    "# 配置验证策略为每个epoch进行验证,并传入评价函数\n",
    "train_args = TrainingArguments('test-trainer', \n",
    "#                                evaluation_strategy='epoch',\n",
    "#                               per_device_train_batch_size=4,\n",
    "                              )\n",
    "\n",
    "# Trainer 进行精调\n",
    "trainer = Trainer(model,\n",
    "                 train_args,\n",
    "                 train_dataset=tokenized_datasets['train'],\n",
    "                 eval_dataset=tokenized_datasets['validation'],\n",
    "                 data_collator=data_collator, #  将一批样本进行整理，转换为pyorch tensor/padding/连接\n",
    "                 tokenizer=tokenizer,\n",
    "                 compute_metrics=compute_metrics)\n",
    "\n",
    "# 训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae1deb4e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6336934566497803,\n",
       " 'eval_accuracy': 0.8578431372549019,\n",
       " 'eval_f1': 0.9003436426116839,\n",
       " 'eval_runtime': 8.8745,\n",
       " 'eval_samples_per_second': 45.974,\n",
       " 'eval_steps_per_second': 5.747,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估\n",
    "import numpy as np\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c094732",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output_dir\n",
      "Configuration saved in output_dir\\config.json\n",
      "Model weights saved in output_dir\\pytorch_model.bin\n",
      "tokenizer config file saved in output_dir\\tokenizer_config.json\n",
      "Special tokens file saved in output_dir\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "trainer.save_model(output_dir=\"output_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d5cd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1725\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad3f463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/lizhong/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7f5e11cea84bb1ab1e65a4a9d5ca51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sentence1': \"PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So .\",\n",
       " 'sentence2': 'Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "# 加载数据\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# 加载分词编码工具\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "dataset_test = raw_datasets['test']\n",
    "del raw_datasets\n",
    "dataset_test\n",
    "dataset_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7467e1c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method batch_encode_plus in module transformers.tokenization_utils_base:\n",
      "\n",
      "batch_encode_plus(batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.bert.tokenization_bert_fast.BertTokenizerFast instance\n",
      "    Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      "    \n",
      "    <Tip warning={true}>\n",
      "    \n",
      "    This method is deprecated, `__call__` should be used instead.\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Args:\n",
      "        batch_text_or_text_pairs (`List[str]`, `List[Tuple[str, str]]`, `List[List[str]]`, `List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also `List[List[int]]`, `List[Tuple[List[int], List[int]]]`):\n",
      "            Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      "            string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      "            details in `encode_plus`).\n",
      "    \n",
      "        add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to encode the sequences with the special tokens relative to their model.\n",
      "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      "            Activates and controls padding. Accepts the following values:\n",
      "    \n",
      "            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      "              sequence if provided).\n",
      "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      "              acceptable input length for the model if that argument is not provided.\n",
      "            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      "              lengths).\n",
      "        truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      "            Activates and controls truncation. Accepts the following values:\n",
      "    \n",
      "            - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      "              to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      "              truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      "              sequences (or a batch of pairs) is provided.\n",
      "            - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "              truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "            - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "              truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "            - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      "              greater than the model maximum admissible input size).\n",
      "        max_length (`int`, *optional*):\n",
      "            Controls the maximum length to use by one of the truncation/padding parameters.\n",
      "    \n",
      "            If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      "            is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      "            length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "        stride (`int`, *optional*, defaults to 0):\n",
      "            If set to a number along with `max_length`, the overflowing tokens returned when\n",
      "            `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      "            returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      "            argument defines the number of overlapping tokens.\n",
      "        is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      "            tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      "            which it will tokenize. This is useful for NER or token classification.\n",
      "        pad_to_multiple_of (`int`, *optional*):\n",
      "            If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      "            the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      "        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "            If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "    \n",
      "            - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      "            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "            - `'np'`: Return Numpy `np.ndarray` objects.\n",
      "    \n",
      "        return_token_type_ids (`bool`, *optional*):\n",
      "            Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      "            the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "    \n",
      "            [What are token type IDs?](../glossary#token-type-ids)\n",
      "        return_attention_mask (`bool`, *optional*):\n",
      "            Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "            to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "    \n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "        return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      "            of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      "            of returning overflowing tokens.\n",
      "        return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return special tokens mask information.\n",
      "        return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return `(char_start, char_end)` for each token.\n",
      "    \n",
      "            This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      "            Python's tokenizer, this method will raise `NotImplementedError`.\n",
      "        return_length  (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return the lengths of the encoded inputs.\n",
      "        verbose (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to print more information and warnings.\n",
      "        **kwargs: passed to the `self.tokenize()` method\n",
      "    \n",
      "    Return:\n",
      "        [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      "    \n",
      "        - **input_ids** -- List of token ids to be fed to a model.\n",
      "    \n",
      "          [What are input IDs?](../glossary#input-ids)\n",
      "    \n",
      "        - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      "          if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      "    \n",
      "          [What are token type IDs?](../glossary#token-type-ids)\n",
      "    \n",
      "        - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      "          `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      "    \n",
      "          [What are attention masks?](../glossary#attention-mask)\n",
      "    \n",
      "        - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      "          `return_overflowing_tokens=True`).\n",
      "        - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      "          `return_overflowing_tokens=True`).\n",
      "        - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      "          regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      "        - **length** -- The length of the inputs (when `return_length=True`)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.batch_encode_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff98038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x23947e18040>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 批处理函数\n",
    "def collate_fn(data):\n",
    "#     print(len(data), data[0])\n",
    "    labels = [i['label'] for i in data]\n",
    "    sents = [(i['sentence1'], i['sentence2']) for i in data]\n",
    "#     print(labels)\n",
    "#     print(sents)\n",
    "    \n",
    "    \n",
    "    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                       truncation=True,\n",
    "                                       padding=\"max_length\",\n",
    "                                       max_length=50,\n",
    "                                       return_tensors='pt')\n",
    "    \n",
    "    input_ids = data['input_ids']\n",
    "    token_type_ids = data['attention_mask']\n",
    "    attention_mask = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "    return labels, input_ids, token_type_ids, attention_mask\n",
    "\n",
    "# 数据加载器\n",
    "loader_test = torch.utils.data.DataLoader(dataset=dataset_test,\n",
    "                                          batch_size=4,\n",
    "                                          collate_fn=collate_fn,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4244f85",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0批数据: 0.5\n",
      "第1批数据: 0.375\n",
      "第2批数据: 0.4166666666666667\n",
      "第3批数据: 0.375\n",
      "第4批数据: 0.45\n",
      "第5批数据: 0.375\n",
      "第6批数据: 0.39285714285714285\n",
      "第7批数据: 0.375\n",
      "第8批数据: 0.3888888888888889\n",
      "第9批数据: 0.425\n",
      "第10批数据: 0.4090909090909091\n",
      "第11批数据: 0.375\n",
      "第12批数据: 0.36538461538461536\n",
      "第13批数据: 0.35714285714285715\n",
      "第14批数据: 0.36666666666666664\n",
      "第15批数据: 0.375\n",
      "第16批数据: 0.39705882352941174\n",
      "第17批数据: 0.375\n",
      "第18批数据: 0.3815789473684211\n",
      "第19批数据: 0.3625\n",
      "第20批数据: 0.35714285714285715\n",
      "第21批数据: 0.3409090909090909\n",
      "第22批数据: 0.33695652173913043\n",
      "第23批数据: 0.3541666666666667\n",
      "第24批数据: 0.36\n",
      "第25批数据: 0.34615384615384615\n",
      "第26批数据: 0.3425925925925926\n",
      "第27批数据: 0.3482142857142857\n",
      "第28批数据: 0.3448275862068966\n",
      "第29批数据: 0.3416666666666667\n",
      "第30批数据: 0.3548387096774194\n",
      "第31批数据: 0.359375\n",
      "第32批数据: 0.36363636363636365\n",
      "第33批数据: 0.3602941176470588\n",
      "第34批数据: 0.37142857142857144\n",
      "第35批数据: 0.375\n",
      "第36批数据: 0.3716216216216216\n",
      "第37批数据: 0.3684210526315789\n",
      "第38批数据: 0.3717948717948718\n",
      "第39批数据: 0.36875\n",
      "第40批数据: 0.3719512195121951\n",
      "第41批数据: 0.36904761904761907\n",
      "第42批数据: 0.36627906976744184\n",
      "第43批数据: 0.36363636363636365\n",
      "第44批数据: 0.37777777777777777\n",
      "第45批数据: 0.3858695652173913\n",
      "第46批数据: 0.3882978723404255\n",
      "第47批数据: 0.3958333333333333\n",
      "第48批数据: 0.3877551020408163\n",
      "第49批数据: 0.38\n",
      "第50批数据: 0.37745098039215685\n",
      "第51批数据: 0.375\n",
      "第52批数据: 0.37264150943396224\n",
      "第53批数据: 0.36574074074074076\n",
      "第54批数据: 0.37272727272727274\n",
      "第55批数据: 0.36607142857142855\n",
      "第56批数据: 0.36403508771929827\n",
      "第57批数据: 0.36637931034482757\n",
      "第58批数据: 0.3686440677966102\n",
      "第59批数据: 0.375\n",
      "第60批数据: 0.38114754098360654\n",
      "第61批数据: 0.38306451612903225\n",
      "第62批数据: 0.38095238095238093\n",
      "第63批数据: 0.375\n",
      "第64批数据: 0.3769230769230769\n",
      "第65批数据: 0.3787878787878788\n",
      "第66批数据: 0.3805970149253731\n",
      "第67批数据: 0.38235294117647056\n",
      "第68批数据: 0.3804347826086957\n",
      "第69批数据: 0.375\n",
      "第70批数据: 0.36971830985915494\n",
      "第71批数据: 0.3680555555555556\n",
      "第72批数据: 0.3664383561643836\n",
      "第73批数据: 0.3614864864864865\n",
      "第74批数据: 0.36666666666666664\n",
      "第75批数据: 0.3684210526315789\n",
      "第76批数据: 0.37012987012987014\n",
      "第77批数据: 0.3717948717948718\n",
      "第78批数据: 0.370253164556962\n",
      "第79批数据: 0.375\n",
      "第80批数据: 0.37037037037037035\n",
      "第81批数据: 0.3719512195121951\n",
      "第82批数据: 0.37349397590361444\n",
      "第83批数据: 0.37202380952380953\n",
      "第84批数据: 0.37058823529411766\n",
      "第85批数据: 0.3691860465116279\n",
      "第86批数据: 0.3706896551724138\n",
      "第87批数据: 0.3693181818181818\n",
      "第88批数据: 0.3651685393258427\n",
      "第89批数据: 0.3638888888888889\n",
      "第90批数据: 0.36538461538461536\n",
      "第91批数据: 0.3641304347826087\n",
      "第92批数据: 0.3629032258064516\n",
      "第93批数据: 0.36436170212765956\n",
      "第94批数据: 0.36578947368421055\n",
      "第95批数据: 0.3619791666666667\n",
      "第96批数据: 0.36082474226804123\n",
      "第97批数据: 0.35714285714285715\n",
      "第98批数据: 0.35858585858585856\n",
      "第99批数据: 0.355\n",
      "第100批数据: 0.3589108910891089\n",
      "第101批数据: 0.3627450980392157\n",
      "第102批数据: 0.36650485436893204\n",
      "第103批数据: 0.36538461538461536\n",
      "第104批数据: 0.36428571428571427\n",
      "第105批数据: 0.3632075471698113\n",
      "第106批数据: 0.3598130841121495\n",
      "第107批数据: 0.3611111111111111\n",
      "第108批数据: 0.3623853211009174\n",
      "第109批数据: 0.3613636363636364\n",
      "第110批数据: 0.36261261261261263\n",
      "第111批数据: 0.36607142857142855\n",
      "第112批数据: 0.36504424778761063\n",
      "第113批数据: 0.3618421052631579\n",
      "第114批数据: 0.36086956521739133\n",
      "第115批数据: 0.3599137931034483\n",
      "第116批数据: 0.35683760683760685\n",
      "第117批数据: 0.3580508474576271\n",
      "第118批数据: 0.35714285714285715\n",
      "第119批数据: 0.35833333333333334\n",
      "第120批数据: 0.3574380165289256\n",
      "第121批数据: 0.35860655737704916\n",
      "第122批数据: 0.3597560975609756\n",
      "第123批数据: 0.36088709677419356\n",
      "第124批数据: 0.36\n",
      "第125批数据: 0.3611111111111111\n",
      "第126批数据: 0.36023622047244097\n",
      "第127批数据: 0.359375\n",
      "第128批数据: 0.36046511627906974\n",
      "第129批数据: 0.3596153846153846\n",
      "第130批数据: 0.36259541984732824\n",
      "第131批数据: 0.36174242424242425\n",
      "第132批数据: 0.36466165413533835\n",
      "第133批数据: 0.3619402985074627\n",
      "第134批数据: 0.362962962962963\n",
      "第135批数据: 0.36580882352941174\n",
      "第136批数据: 0.36496350364963503\n",
      "第137批数据: 0.36231884057971014\n",
      "第138批数据: 0.36330935251798563\n",
      "第139批数据: 0.36607142857142855\n",
      "第140批数据: 0.3670212765957447\n",
      "第141批数据: 0.3644366197183099\n",
      "第142批数据: 0.36538461538461536\n",
      "第143批数据: 0.3663194444444444\n",
      "第144批数据: 0.36551724137931035\n",
      "第145批数据: 0.3647260273972603\n",
      "第146批数据: 0.3656462585034014\n",
      "第147批数据: 0.36486486486486486\n",
      "第148批数据: 0.3640939597315436\n",
      "第149批数据: 0.36333333333333334\n",
      "第150批数据: 0.36258278145695366\n",
      "第151批数据: 0.3651315789473684\n",
      "第152批数据: 0.3660130718954248\n",
      "第153批数据: 0.36688311688311687\n",
      "第154批数据: 0.36774193548387096\n",
      "第155批数据: 0.36538461538461536\n",
      "第156批数据: 0.3646496815286624\n",
      "第157批数据: 0.36550632911392406\n",
      "第158批数据: 0.36792452830188677\n",
      "第159批数据: 0.3671875\n",
      "第160批数据: 0.36801242236024845\n",
      "第161批数据: 0.36728395061728397\n",
      "第162批数据: 0.3696319018404908\n",
      "第163批数据: 0.3704268292682927\n",
      "第164批数据: 0.37272727272727274\n",
      "第165批数据: 0.3704819277108434\n",
      "第166批数据: 0.3712574850299401\n",
      "第167批数据: 0.3705357142857143\n",
      "第168批数据: 0.3698224852071006\n",
      "第169批数据: 0.37058823529411766\n",
      "第170批数据: 0.3684210526315789\n",
      "第171批数据: 0.3691860465116279\n",
      "第172批数据: 0.3684971098265896\n",
      "第173批数据: 0.36637931034482757\n",
      "第174批数据: 0.3657142857142857\n",
      "第175批数据: 0.3650568181818182\n",
      "第176批数据: 0.3644067796610169\n",
      "第177批数据: 0.3651685393258427\n",
      "第178批数据: 0.3659217877094972\n",
      "第179批数据: 0.36527777777777776\n",
      "第180批数据: 0.3632596685082873\n",
      "第181批数据: 0.3626373626373626\n",
      "第182批数据: 0.3620218579234973\n",
      "第183批数据: 0.360054347826087\n",
      "第184批数据: 0.35945945945945945\n",
      "第185批数据: 0.3575268817204301\n",
      "第186批数据: 0.35561497326203206\n",
      "第187批数据: 0.3537234042553192\n",
      "第188批数据: 0.3531746031746032\n",
      "第189批数据: 0.3539473684210526\n",
      "第190批数据: 0.35340314136125656\n",
      "第191批数据: 0.3541666666666667\n",
      "第192批数据: 0.3536269430051813\n",
      "第193批数据: 0.35309278350515466\n",
      "第194批数据: 0.35512820512820514\n",
      "第195批数据: 0.35331632653061223\n",
      "第196批数据: 0.35279187817258884\n",
      "第197批数据: 0.3522727272727273\n",
      "第198批数据: 0.3542713567839196\n",
      "第199批数据: 0.3525\n",
      "第200批数据: 0.35323383084577115\n",
      "第201批数据: 0.3527227722772277\n",
      "第202批数据: 0.35344827586206895\n",
      "第203批数据: 0.3541666666666667\n",
      "第204批数据: 0.3548780487804878\n",
      "第205批数据: 0.3567961165048544\n",
      "第206批数据: 0.358695652173913\n",
      "第207批数据: 0.3581730769230769\n",
      "第208批数据: 0.3576555023923445\n",
      "第209批数据: 0.35714285714285715\n",
      "第210批数据: 0.3566350710900474\n",
      "第211批数据: 0.3584905660377358\n",
      "第212批数据: 0.357981220657277\n",
      "第213批数据: 0.35630841121495327\n",
      "第214批数据: 0.3558139534883721\n",
      "第215批数据: 0.3541666666666667\n",
      "第216批数据: 0.35368663594470046\n",
      "第217批数据: 0.3543577981651376\n",
      "第218批数据: 0.3527397260273973\n",
      "第219批数据: 0.3534090909090909\n",
      "第220批数据: 0.35294117647058826\n",
      "第221批数据: 0.3536036036036036\n",
      "第222批数据: 0.3531390134529148\n",
      "第223批数据: 0.3515625\n",
      "第224批数据: 0.3522222222222222\n",
      "第225批数据: 0.35176991150442477\n",
      "第226批数据: 0.3524229074889868\n",
      "第227批数据: 0.3519736842105263\n",
      "第228批数据: 0.35152838427947597\n",
      "第229批数据: 0.35108695652173916\n",
      "第230批数据: 0.3495670995670996\n",
      "第231批数据: 0.3480603448275862\n",
      "第232批数据: 0.3487124463519313\n",
      "第233批数据: 0.3482905982905983\n",
      "第234批数据: 0.3478723404255319\n",
      "第235批数据: 0.3474576271186441\n",
      "第236批数据: 0.34810126582278483\n",
      "第237批数据: 0.3487394957983193\n",
      "第238批数据: 0.348326359832636\n",
      "第239批数据: 0.346875\n",
      "第240批数据: 0.3454356846473029\n",
      "第241批数据: 0.34607438016528924\n",
      "第242批数据: 0.3446502057613169\n",
      "第243批数据: 0.3442622950819672\n",
      "第244批数据: 0.3438775510204082\n",
      "第245批数据: 0.3434959349593496\n",
      "第246批数据: 0.3431174089068826\n",
      "第247批数据: 0.34375\n",
      "第248批数据: 0.3433734939759036\n",
      "第249批数据: 0.344\n",
      "第250批数据: 0.3436254980079681\n",
      "第251批数据: 0.34424603174603174\n",
      "第252批数据: 0.3448616600790514\n",
      "第253批数据: 0.34448818897637795\n",
      "第254批数据: 0.34411764705882353\n",
      "第255批数据: 0.3447265625\n",
      "第256批数据: 0.3453307392996109\n",
      "第257批数据: 0.3449612403100775\n",
      "第258批数据: 0.34555984555984554\n",
      "第259批数据: 0.34615384615384615\n",
      "第260批数据: 0.3457854406130268\n",
      "第261批数据: 0.3463740458015267\n",
      "第262批数据: 0.34600760456273766\n",
      "第263批数据: 0.3446969696969697\n",
      "第264批数据: 0.3433962264150943\n",
      "第265批数据: 0.3430451127819549\n",
      "第266批数据: 0.34269662921348315\n",
      "第267批数据: 0.34328358208955223\n",
      "第268批数据: 0.3420074349442379\n",
      "第269批数据: 0.3416666666666667\n",
      "第270批数据: 0.3413284132841328\n",
      "第271批数据: 0.3428308823529412\n",
      "第272批数据: 0.3424908424908425\n",
      "第273批数据: 0.34124087591240876\n",
      "第274批数据: 0.34\n",
      "第275批数据: 0.33967391304347827\n",
      "第276批数据: 0.34025270758122744\n",
      "第277批数据: 0.3408273381294964\n",
      "第278批数据: 0.34139784946236557\n",
      "第279批数据: 0.3410714285714286\n",
      "第280批数据: 0.3407473309608541\n",
      "第281批数据: 0.34131205673758863\n",
      "第282批数据: 0.3418727915194346\n",
      "第283批数据: 0.3415492957746479\n",
      "第284批数据: 0.3429824561403509\n",
      "第285批数据: 0.34353146853146854\n",
      "第286批数据: 0.343205574912892\n",
      "第287批数据: 0.34375\n",
      "第288批数据: 0.34429065743944637\n",
      "第289批数据: 0.3439655172413793\n",
      "第290批数据: 0.3427835051546392\n",
      "第291批数据: 0.3416095890410959\n",
      "第292批数据: 0.34044368600682595\n",
      "第293批数据: 0.3392857142857143\n",
      "第294批数据: 0.34067796610169493\n",
      "第295批数据: 0.34121621621621623\n",
      "第296批数据: 0.34175084175084175\n",
      "第297批数据: 0.3414429530201342\n",
      "第298批数据: 0.3411371237458194\n",
      "第299批数据: 0.3416666666666667\n",
      "第300批数据: 0.34219269102990035\n",
      "第301批数据: 0.34188741721854304\n",
      "第302批数据: 0.3415841584158416\n",
      "第303批数据: 0.3412828947368421\n",
      "第304批数据: 0.3434426229508197\n",
      "第305批数据: 0.3423202614379085\n",
      "第306批数据: 0.34364820846905536\n",
      "第307批数据: 0.34415584415584416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第308批数据: 0.3454692556634304\n",
      "第309批数据: 0.3443548387096774\n",
      "第310批数据: 0.3432475884244373\n",
      "第311批数据: 0.3421474358974359\n",
      "第312批数据: 0.3426517571884984\n",
      "第313批数据: 0.3431528662420382\n",
      "第314批数据: 0.34365079365079365\n",
      "第315批数据: 0.34414556962025317\n",
      "第316批数据: 0.3438485804416404\n",
      "第317批数据: 0.34355345911949686\n",
      "第318批数据: 0.3440438871473354\n",
      "第319批数据: 0.34375\n",
      "第320批数据: 0.3442367601246106\n",
      "第321批数据: 0.3447204968944099\n",
      "第322批数据: 0.34365325077399383\n",
      "第323批数据: 0.3449074074074074\n",
      "第324批数据: 0.3453846153846154\n",
      "第325批数据: 0.3458588957055215\n",
      "第326批数据: 0.3463302752293578\n",
      "第327批数据: 0.34603658536585363\n",
      "第328批数据: 0.34498480243161095\n",
      "第329批数据: 0.34393939393939393\n",
      "第330批数据: 0.34365558912386707\n",
      "第331批数据: 0.3433734939759036\n",
      "第332批数据: 0.3430930930930931\n",
      "第333批数据: 0.34281437125748504\n",
      "第334批数据: 0.3417910447761194\n",
      "第335批数据: 0.34226190476190477\n",
      "第336批数据: 0.34124629080118696\n",
      "第337批数据: 0.34097633136094674\n",
      "第338批数据: 0.3407079646017699\n",
      "第339批数据: 0.34044117647058825\n",
      "第340批数据: 0.34017595307917886\n",
      "第341批数据: 0.3399122807017544\n",
      "第342批数据: 0.3403790087463557\n",
      "第343批数据: 0.34084302325581395\n",
      "第344批数据: 0.34057971014492755\n",
      "第345批数据: 0.33959537572254334\n",
      "第346批数据: 0.3400576368876081\n",
      "第347批数据: 0.3390804597701149\n",
      "第348批数据: 0.33882521489971346\n",
      "第349批数据: 0.3392857142857143\n",
      "第350批数据: 0.34045584045584043\n",
      "第351批数据: 0.3409090909090909\n",
      "第352批数据: 0.34206798866855526\n",
      "第353批数据: 0.3425141242937853\n",
      "第354批数据: 0.3415492957746479\n",
      "第355批数据: 0.3433988764044944\n",
      "第356批数据: 0.3431372549019608\n",
      "第357批数据: 0.3435754189944134\n",
      "第358批数据: 0.3426183844011142\n",
      "第359批数据: 0.34305555555555556\n",
      "第360批数据: 0.34418282548476453\n",
      "第361批数据: 0.3446132596685083\n",
      "第362批数据: 0.34366391184573003\n",
      "第363批数据: 0.3434065934065934\n",
      "第364批数据: 0.34383561643835614\n",
      "第365批数据: 0.3435792349726776\n",
      "第366批数据: 0.34332425068119893\n",
      "第367批数据: 0.34375\n",
      "第368批数据: 0.34417344173441733\n",
      "第369批数据: 0.34459459459459457\n",
      "第370批数据: 0.3436657681940701\n",
      "第371批数据: 0.34341397849462363\n",
      "第372批数据: 0.34316353887399464\n",
      "第373批数据: 0.3429144385026738\n",
      "第374批数据: 0.3433333333333333\n",
      "第375批数据: 0.34242021276595747\n",
      "第376批数据: 0.3421750663129973\n",
      "第377批数据: 0.3425925925925926\n",
      "第378批数据: 0.34366754617414247\n",
      "第379批数据: 0.3427631578947368\n",
      "第380批数据: 0.3431758530183727\n",
      "第381批数据: 0.34293193717277487\n",
      "第382批数据: 0.3433420365535248\n",
      "第383批数据: 0.3430989583333333\n",
      "第384批数据: 0.34415584415584416\n",
      "第385批数据: 0.3439119170984456\n",
      "第386批数据: 0.3449612403100775\n",
      "第387批数据: 0.3440721649484536\n",
      "第388批数据: 0.3444730077120823\n",
      "第389批数据: 0.34487179487179487\n",
      "第390批数据: 0.3452685421994885\n",
      "第391批数据: 0.3450255102040816\n",
      "第392批数据: 0.34478371501272265\n",
      "第393批数据: 0.34390862944162437\n",
      "第394批数据: 0.3430379746835443\n",
      "第395批数据: 0.3428030303030303\n",
      "第396批数据: 0.3431989924433249\n",
      "第397批数据: 0.34296482412060303\n",
      "第398批数据: 0.3433583959899749\n",
      "第399批数据: 0.34375\n",
      "第400批数据: 0.34351620947630923\n",
      "第401批数据: 0.34266169154228854\n",
      "第402批数据: 0.34181141439205953\n",
      "第403批数据: 0.3415841584158416\n",
      "第404批数据: 0.3425925925925926\n",
      "第405批数据: 0.3435960591133005\n",
      "第406批数据: 0.34336609336609336\n",
      "第407批数据: 0.3425245098039216\n",
      "第408批数据: 0.3422982885085575\n",
      "第409批数据: 0.34146341463414637\n",
      "第410批数据: 0.3418491484184915\n",
      "第411批数据: 0.341626213592233\n",
      "第412批数据: 0.3414043583535109\n",
      "第413批数据: 0.34118357487922707\n",
      "第414批数据: 0.34156626506024096\n",
      "第415批数据: 0.34134615384615385\n",
      "第416批数据: 0.3411270983213429\n",
      "第417批数据: 0.3415071770334928\n",
      "第418批数据: 0.3412887828162291\n",
      "第419批数据: 0.3416666666666667\n",
      "第420批数据: 0.3414489311163896\n",
      "第421批数据: 0.3412322274881517\n",
      "第422批数据: 0.3416075650118203\n",
      "第423批数据: 0.34139150943396224\n",
      "第424批数据: 0.34058823529411764\n",
      "第425批数据: 0.3397887323943662\n",
      "第426批数据: 0.3401639344262295\n",
      "第427批数据: 0.3405373831775701\n",
      "第428批数据: 0.34032634032634035\n",
      "第429批数据: 0.34069767441860466\n",
      "第430批数据: 0.34106728538283065\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    # 加载模型\n",
    "    model_test = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                          num_labels=2)\n",
    "    # 加载参数\n",
    "    model_test.load_state_dict(torch.load('output_dir/pytorch_model.bin'))\n",
    "    model_test.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i,(labels, input_ids, token_type_ids, attention_mask) in enumerate(loader_test):\n",
    "        with torch.no_grad():\n",
    "            out = model_test(input_ids=input_ids,\n",
    "                       token_type_ids=token_type_ids,\n",
    "                       attention_mask=attention_mask)\n",
    "            \n",
    "        out = out['logits'].argmax(dim=-1)\n",
    "        correct += (out==labels).sum().item()\n",
    "        total += len(labels)\n",
    "        print(f'第{i}批数据:', correct/total)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"start...\")\n",
    "    test()\n",
    "    print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4395552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModelForSequenceClassification\t AutoTokenizer\t checkpoint\t collate_fn\t dataset_test\t load_dataset\t loader_test\t model_test\t test\t \n",
      "tokenize_function\t tokenizer\t torch\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51eb3b8b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
      "     -------------------------------------- 365.3/365.3 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.9.2-py3-none-any.whl (210 kB)\n",
      "     -------------------------------------- 210.6/210.6 kB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\software\\anaconda3\\lib\\site-packages (from optuna) (21.3)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tqdm in c:\\software\\anaconda3\\lib\\site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: PyYAML in c:\\software\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: numpy in c:\\software\\anaconda3\\lib\\site-packages (from optuna) (1.21.5)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\software\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.7/78.7 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\software\\anaconda3\\lib\\site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\software\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\software\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\software\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
      "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.9.2 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n",
      "Collecting ray[tune]\n",
      "  Downloading ray-2.2.0-cp39-cp39-win_amd64.whl (20.8 MB)\n",
      "     --------------------------------------- 20.8/20.8 MB 28.4 MB/s eta 0:00:00\n",
      "Collecting virtualenv>=20.0.24\n",
      "  Downloading virtualenv-20.17.1-py3-none-any.whl (8.8 MB)\n",
      "     ---------------------------------------- 8.8/8.8 MB 35.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\software\\anaconda3\\lib\\site-packages (from ray[tune]) (1.21.5)\n",
      "Requirement already satisfied: pyyaml in c:\\software\\anaconda3\\lib\\site-packages (from ray[tune]) (6.0)\n",
      "Requirement already satisfied: frozenlist in c:\\software\\anaconda3\\lib\\site-packages (from ray[tune]) (1.3.3)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\software\\anaconda3\\lib\\site-packages (from ray[tune]) (1.0.3)\n",
      "Requirement already satisfied: click>=7.0 in c:\\software\\anaconda3\\lib\\site-packages (from ray[tune]) (8.0.4)\n",
      "Requirement already satisfied: attrs in c:\\users\\lizhong\\appdata\\roaming\\python\\python39\\site-packages (from ray[tune]) (22.2.0)\n",
      "Requirement already satisfied: requests in c:\\software\\anaconda3\\lib\\site-packages (from ray[tune]) (2.28.1)\n",
      "Collecting protobuf!=3.19.5,>=3.15.3\n",
      "  Downloading protobuf-4.21.12-cp39-cp39-win_amd64.whl (527 kB)\n",
      "     ------------------------------------- 527.0/527.0 kB 34.5 MB/s eta 0:00:00\n",
      "Collecting grpcio>=1.32.0\n",
      "  Downloading grpcio-1.51.1-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 39.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonschema in c:\\software\\anaconda3\\lib\\site-packages (from ray[tune]) (4.16.0)\n",
      "Requirement already satisfied: aiosignal in c:\\users\\lizhong\\appdata\\roaming\\python\\python39\\site-packages (from ray[tune]) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\software\\anaconda3\\lib\\site-packages (from ray[tune]) (3.6.0)\n",
      "Collecting tensorboardX>=1.9\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "     ---------------------------------------- 125.4/125.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\lizhong\\appdata\\roaming\\python\\python39\\site-packages (from ray[tune]) (1.5.2)\n",
      "Requirement already satisfied: tabulate in c:\\software\\anaconda3\\lib\\site-packages (from ray[tune]) (0.8.10)\n",
      "Requirement already satisfied: colorama in c:\\software\\anaconda3\\lib\\site-packages (from click>=7.0->ray[tune]) (0.4.5)\n",
      "Collecting protobuf!=3.19.5,>=3.15.3\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-win_amd64.whl (904 kB)\n",
      "     ------------------------------------- 904.1/904.1 kB 59.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in c:\\software\\anaconda3\\lib\\site-packages (from virtualenv>=20.0.24->ray[tune]) (2.5.2)\n",
      "Collecting distlib<1,>=0.3.6\n",
      "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "     ------------------------------------- 468.5/468.5 kB 28.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\software\\anaconda3\\lib\\site-packages (from jsonschema->ray[tune]) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\software\\anaconda3\\lib\\site-packages (from pandas->ray[tune]) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\software\\anaconda3\\lib\\site-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\software\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\software\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\software\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\software\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (2022.9.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\software\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->ray[tune]) (1.16.0)\n",
      "Installing collected packages: distlib, virtualenv, protobuf, grpcio, tensorboardX, ray\n",
      "Successfully installed distlib-0.3.6 grpcio-1.51.1 protobuf-3.20.1 ray-2.2.0 tensorboardX-2.5.1 virtualenv-20.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna\n",
    "!pip install ray[tune]\n",
    "!pip install sigopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c01d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数搜索\n",
    "\n",
    "# 3.数据预处理\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 加载数据\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# 加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# 数据处理\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], \n",
    "                     example[\"sentence2\"], \n",
    "                     truncation=True)  # return_tensors='pt'\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # 动态padding，\n",
    "\n",
    "# 包装成一个函数\n",
    "def compute_metrics(eval_preds: tuple):\n",
    "    metric = load_metric('glue', 'mrpc')\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions,\n",
    "                         references=labels)\n",
    "\n",
    "# 配置验证策略为每个epoch进行验证,并传入评价函数\n",
    "train_args = TrainingArguments('test-trainer', evaluation_strategy='epoch')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint,  num_labels=2)\n",
    "\n",
    "# Trainer 进行精调\n",
    "trainer = Trainer(model_init=model_init,\n",
    "                 args=train_args,\n",
    "                 train_dataset=tokenized_datasets['train'][:100],\n",
    "                 eval_dataset=tokenized_datasets['validation'],\n",
    "                 data_collator=data_collator, #  将一批样本进行整理，转换为pyorch tensor/padding/连接\n",
    "                 tokenizer=tokenizer,\n",
    "                 compute_metrics=compute_metrics)\n",
    "\n",
    "# 搜索\n",
    "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65edbf07",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_metric in module datasets.load:\n",
      "\n",
      "load_metric(path: str, config_name: Optional[str] = None, process_id: int = 0, num_process: int = 1, cache_dir: Optional[str] = None, experiment_id: Optional[str] = None, keep_in_memory: bool = False, download_config: Optional[datasets.download.download_config.DownloadConfig] = None, download_mode: Optional[datasets.download.download_manager.DownloadMode] = None, revision: Union[str, datasets.utils.version.Version, NoneType] = None, **metric_init_kwargs) -> datasets.metric.Metric\n",
      "    Load a `datasets.Metric`.\n",
      "    \n",
      "    <Deprecated version=\"2.5.0\">\n",
      "    \n",
      "    Use `evaluate.load` instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "    \n",
      "    </Deprecated>\n",
      "    \n",
      "    Args:\n",
      "    \n",
      "        path (``str``):\n",
      "            path to the metric processing script with the metric builder. Can be either:\n",
      "                - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n",
      "                    e.g. ``'./metrics/rouge'`` or ``'./metrics/rogue/rouge.py'``\n",
      "                - a metric identifier on the HuggingFace datasets repo (list all available metrics with ``datasets.list_metrics()``)\n",
      "                    e.g. ``'rouge'`` or ``'bleu'``\n",
      "        config_name (:obj:`str`, optional): selecting a configuration for the metric (e.g. the GLUE metric has a configuration for each subset)\n",
      "        process_id (:obj:`int`, optional): for distributed evaluation: id of the process\n",
      "        num_process (:obj:`int`, optional): for distributed evaluation: total number of processes\n",
      "        cache_dir (Optional str): path to store the temporary predictions and references (default to `~/.cache/huggingface/metrics/`)\n",
      "        experiment_id (``str``): A specific experiment id. This is used if several distributed evaluations share the same file system.\n",
      "            This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\n",
      "        keep_in_memory (bool): Whether to store the temporary results in memory (defaults to False)\n",
      "        download_config (Optional ``datasets.DownloadConfig``: specific download configuration parameters.\n",
      "        download_mode (:class:`DownloadMode`, default ``REUSE_DATASET_IF_EXISTS``): Download/generate mode.\n",
      "        revision (Optional ``Union[str, datasets.Version]``): if specified, the module will be loaded from the datasets repository\n",
      "            at this version. By default, it is set to the local version of the lib. Specifying a version that is different from\n",
      "            your local version of the lib might cause compatibility issues.\n",
      "    \n",
      "    Returns:\n",
      "        `datasets.Metric`\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_metric\n",
      "    >>> accuracy = load_metric('accuracy')\n",
      "    >>> accuracy.compute(references=[1, 0], predictions=[1, 1])\n",
      "    {'accuracy': 0.5}\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 设置Trainer 为搜索到地最好参数，进行训练\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
