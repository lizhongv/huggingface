{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3362a434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset chn_senti_corp (/home/codespace/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n",
      "Loading cached processed dataset at /home/codespace/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85/cache-d23ef8c490aad355.arrow\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "0 0.7801969051361084 0.25\n",
      "5 0.7143212556838989 0.375\n",
      "10 0.4878574013710022 1.0\n",
      "15 0.5834335684776306 0.75\n",
      "20 0.5357387065887451 0.75\n",
      "25 0.5131847858428955 0.75\n",
      "30 0.4711150527000427 0.875\n",
      "35 0.4557431936264038 0.875\n",
      "40 0.512300968170166 0.75\n",
      "45 0.5064978003501892 0.625\n",
      "50 0.396485298871994 0.875\n",
      "55 0.5364752411842346 0.75\n",
      "60 0.5696155428886414 0.75\n",
      "65 0.45890069007873535 0.875\n",
      "70 0.4444369673728943 0.875\n",
      "75 0.6914003491401672 0.5\n",
      "80 0.4821853041648865 0.875\n",
      "85 0.4341668486595154 0.75\n",
      "90 0.3415139317512512 1.0\n",
      "95 0.3460024893283844 1.0\n",
      "100 0.3879810571670532 0.875\n",
      "105 0.37142595648765564 1.0\n",
      "110 0.34169188141822815 1.0\n",
      "115 0.4674598276615143 0.875\n",
      "120 0.4514293968677521 0.875\n",
      "125 0.37706851959228516 1.0\n",
      "130 0.3896777331829071 1.0\n",
      "135 0.4709225296974182 0.75\n",
      "140 0.43113186955451965 0.875\n",
      "145 0.3441063165664673 1.0\n",
      "150 0.36928948760032654 1.0\n",
      "155 0.4450352191925049 0.875\n",
      "160 0.3400208353996277 1.0\n",
      "165 0.45745861530303955 0.75\n",
      "170 0.3964010775089264 0.875\n",
      "175 0.484200119972229 0.875\n",
      "180 0.40566232800483704 1.0\n",
      "185 0.6213068962097168 0.625\n",
      "190 0.3365526795387268 1.0\n",
      "195 0.464516282081604 0.875\n",
      "200 0.3628746569156647 1.0\n",
      "205 0.4016159176826477 1.0\n",
      "210 0.3306834399700165 1.0\n",
      "215 0.5264304876327515 0.75\n",
      "220 0.4613139033317566 0.875\n",
      "225 0.4923488199710846 0.875\n",
      "230 0.5906926393508911 0.625\n",
      "235 0.4653494954109192 0.875\n",
      "240 0.352424293756485 1.0\n",
      "245 0.503187358379364 0.875\n",
      "250 0.5691575407981873 0.75\n",
      "255 0.43047451972961426 0.875\n",
      "260 0.48363587260246277 0.875\n",
      "265 0.37935197353363037 1.0\n",
      "270 0.7138059139251709 0.625\n",
      "275 0.43977072834968567 0.875\n",
      "280 0.5871142745018005 0.625\n",
      "285 0.37708163261413574 1.0\n",
      "290 0.38236892223358154 1.0\n",
      "295 0.5179159045219421 0.75\n",
      "300 0.5012160539627075 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset chn_senti_corp (/home/codespace/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204edc6884174838b8508b99167c6a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "第0批数据: 0.84375\n",
      "1\n",
      "第1批数据: 0.859375\n",
      "2\n",
      "第2批数据: 0.875\n",
      "3\n",
      "第3批数据: 0.875\n",
      "4\n",
      "第4批数据: 0.88125\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1.定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        dataset = load_dataset(path='seamew/ChnSentiCorp', split=split)\n",
    "\n",
    "        def f(data):\n",
    "            return len(data['text']) > 40\n",
    "\n",
    "        self.dataset = dataset.filter(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "\n",
    "        # 切分一句话为前半句和后半句\n",
    "        sentence1 = text[:20]\n",
    "        sentence2 = text[20:40]\n",
    "        label = 0  # 有关为0，无关为1\n",
    "\n",
    "        #有一半的概率把后半句替换为一句无关的话\n",
    "        if random.randint(0, 1) == 0:\n",
    "            j = random.randint(0, len(self.dataset) - 1)\n",
    "            sentence2 = self.dataset[j]['text'][20:40]\n",
    "            label = 1\n",
    "\n",
    "        return sentence1, sentence2, label\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "# sentence1, sentence2, label = dataset[0]\n",
    "# len(dataset), sentence1, sentence2, label\n",
    "\n",
    "# 2. 加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 3. 定义批处理函数\n",
    "def collate_fn(data):\n",
    "    sents = [i[:2] for i in data]\n",
    "    labels = [i[2] for i in data]\n",
    "\n",
    "    # 批处理增强编码函数\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=45,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True,\n",
    "                                   add_special_tokens=True)\n",
    "\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "# 4.数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=8,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "# for i, (input_ids, attention_mask, token_type_ids,\n",
    "#         labels) in enumerate(loader):\n",
    "#     break\n",
    "\n",
    "\n",
    "# 5.加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# 6.定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "# 7. 加载整个模型：预训练+下游任务\n",
    "model = Model().to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练\n",
    "def train():\n",
    "    model.train()\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader):\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 正向传播\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(out, labels)\n",
    "        # 反向传播，计算梯度\n",
    "        loss.backward()\n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "            print(i, loss.item(), accuracy)\n",
    "\n",
    "        if i == 300:\n",
    "            break\n",
    "    torch.save(model.state_dict(), 'model/infer.pt')\n",
    "\n",
    "# 测试\n",
    "def test():\n",
    "    test_model = Model()  # 定义模型\n",
    "    test_model.load_state_dict(torch.load('model/infer.pt'))\n",
    "    test_model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        pred = out.argmax(dim=1)\n",
    "\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "        print(f\"第{i}批数据:\", correct / total)\n",
    "\n",
    "if __name__ ==  \"__main__\":\n",
    "    print(\"start...\")\n",
    "    train()\n",
    "    test()\n",
    "    print('end...')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Jan 18 2023, 00:26:41) [GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
