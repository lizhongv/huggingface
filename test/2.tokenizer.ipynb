{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2aee25",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6001d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练vocab和tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    "    cache_dir=None,\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "# 加载本地预训练tokenizer和model    \n",
    "# 只下载config.json，vocab.txt，pytorch_model.bin 三个文件放在 bert-base-chinses 文件夹\n",
    "# tokenizer = BertTokenizer.from_pretrained('model/bert-base-chinese')\n",
    "# model = BertModel.from_pretrained('model/bert-base-chinese')\n",
    "\n",
    "# 或 BertTokenizer只调用vocab文件，BertModel只调用bin文件即可\n",
    "# tokenizer = BertTokenizer.from_pretrained('model.bert-base-chinese/vocab.txt')\n",
    "# mdoel = BertModel.from_pretrained('model/bert-base-chinese/pytorch_model.bin')\n",
    "\n",
    "\n",
    "sents = [\n",
    "    '选择珠江花园的原因就是方便。',\n",
    "    '笔记本的键盘确实爽。',\n",
    "    '房间太小。其他的都一般。',\n",
    "    '今天才知道这书还有第6卷,真有点郁闷.',\n",
    "    '机器背面似乎被撕了张什么标签，残胶还在。',\n",
    "    '春江潮水连海平，海上明月共潮生。',\n",
    "]\n",
    "\n",
    "tokenizer, sents, tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码句子对\n",
    "out = tokenizer.encode(\n",
    "    text=sents[0],\n",
    "    text_pair=sents[1],\n",
    "\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=30,\n",
    "\n",
    "    add_special_tokens=True,  # CLS,SEP,PAD\n",
    "    \n",
    "    return_tensors=None,\n",
    ")\n",
    "print(out)\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f221a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 增强的编码函数\n",
    "out = tokenizer.encode_plus(\n",
    "    text=sents[0],\n",
    "    text_pair=sents[1],\n",
    "\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=30,\n",
    "\n",
    "    add_special_tokens=True,\n",
    " \n",
    "    return_tensors=None,\n",
    "\n",
    "    return_token_type_ids=True,  # 第一个句子0，第二个句子1，padding为0\n",
    "    return_attention_mask=True,  # padding为0，其余为1\n",
    "    return_special_tokens_mask=True,  # 特殊字符为1，其余为0\n",
    "\n",
    "    # 返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
    "    # return_offsets_mapping=True,\n",
    "\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "\n",
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "tokenizer.decode(out['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb964a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量编码句子\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[sents[0], sents[1], sents[2]],\n",
    "\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=15,\n",
    "\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    return_tensors=None,\n",
    "\n",
    "\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #return_offsets_mapping=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "\n",
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out['input_ids'][0]), tokenizer.decode(out['input_ids'][1]), tokenizer.decode(out['input_ids'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量编码成对的句子\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[(sents[0], sents[1]), \n",
    "                            (sents[2], sents[3]),\n",
    "                            (sents[4], sents[5])\n",
    "                            ],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=30,\n",
    "\n",
    "    add_special_tokens=True,\n",
    " \n",
    "    return_tensors=None,\n",
    "\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #return_offsets_mapping=True,\n",
    "\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "\n",
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out['input_ids'][0]), tokenizer.decode(out['input_ids'][1]), tokenizer.decode(out['input_ids'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc2994",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 获取字典\n",
    "voc = tokenizer.get_vocab()\n",
    "\n",
    "# print(voc, type(voc), len(voc))\n",
    "print('[EOS]' in voc, '月光' in voc, '希望' in voc, '李' in voc, '中' in voc)\n",
    "\n",
    "# 添加新词\n",
    "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
    "\n",
    "# 添加新符号\n",
    "tokenizer.add_special_tokens({'eos_token': '[EOS]'})\n",
    "\n",
    "# 加载字典\n",
    "new_voc = tokenizer.get_vocab()\n",
    "\n",
    "print(len(new_voc), new_voc['月光'], new_voc['希望'], new_voc['[EOS]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7cefad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 编码新添加的词\n",
    "out = tokenizer.encode(\n",
    "    text='月光的新希望[EOS]',\n",
    "    text_pair=None,\n",
    "\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=8,\n",
    "\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=None,\n",
    ")\n",
    "\n",
    "print(out)\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052056f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码方式 BertTokenizer\n",
    "text = '[CLS] 武1松1打11老虎 [SEP] 你在哪 [SEP]'\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text)  # 分词\n",
    "print(\"分词：\", tokenized_text)\n",
    "print()\n",
    "\n",
    "a = tokenizer.convert_tokens_to_ids(tokenized_text) #  方式1:手动添加CLS与SEP\n",
    "print(\"方式1:\",a)\n",
    "print(tokenizer.decode(a))\n",
    "print()\n",
    "\n",
    "# 另一种wordpiece\n",
    "# words = list(text)\n",
    "# a_2 = tokenizer.convert_tokens_to_ids(words)\n",
    "# print(a_2)\n",
    "# print(tokenizer.decode(a_2))\n",
    "# print()\n",
    "\n",
    "b = tokenizer(text=text) # 方式2：返回一个字典(包含id,type,mask)，无须手动添加 CLS 与 SEP (与encode_plus不同点在返回tensor维数)\n",
    "print(\"方式2:\" ,b)\n",
    "print(tokenizer.decode(b['input_ids']))\n",
    "print()\n",
    "\n",
    "c=tokenizer.encode(text=text) # 方式3:只返回ids，无须手动添加CLS与SEP \n",
    "print(\"方式3:\", c)\n",
    "print(tokenizer.decode(c))\n",
    "print()\n",
    "\n",
    "d=tokenizer.encode_plus(text=text,max_length=30,return_tensors='pt') # 方式4:返回一个字典(包含id,type,mask)，无须手动添加CLS与SEP   \n",
    "print(\"方式4:\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a3f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码方式 BertTokenizerFast\n",
    "text = '[CLS] 武1松1打11老虎 [SEP] 你在哪 [SEP]'\n",
    "\n",
    "tokenizerfast = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "\n",
    "tokenized_text = tokenizerfast.tokenize(text)\n",
    "print(\"分词：\", tokenized_text)\n",
    "print()\n",
    "\n",
    "a = tokenizerfast.convert_tokens_to_ids(tokenized_text)\n",
    "print(\"方式1：\", a)\n",
    "print(\"解码：\", tokenizerfast.decode(a))\n",
    "print()\n",
    "\n",
    "# words = list(text)\n",
    "# e = tokenizerfast.convert_tokens_to_ids(words)\n",
    "# print(e)\n",
    "# print(tokenizerfast.decode(e))\n",
    "# print()\n",
    "\n",
    "b = tokenizerfast(text, return_offsets_mapping=True)  # 设置标签对其\n",
    "print(\"方式2\", b)\n",
    "print(tokenizerfast.decode(b['input_ids']))\n",
    "print()\n",
    "\n",
    "c = tokenizerfast.encode(text)\n",
    "print(\"方式3\", c)\n",
    "print(\"解码：\", tokenizerfast.decode(c))\n",
    "print()\n",
    "\n",
    "d = tokenizerfast.encode_plus(text)\n",
    "print(\"方式4：\", d)\n",
    "print(\"解码：\", tokenizerfast.decode(d['input_ids']))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
