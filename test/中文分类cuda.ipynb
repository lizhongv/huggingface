{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b103f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel,BertTokenizer\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f6c379",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#加载预训练模型\n",
    "pretrained_model = BertModel.from_pretrained('bert-base-chinese')\n",
    "#需要移动到cuda上\n",
    "pretrained_model.to(device)\n",
    "\n",
    "#不训练,反向传播时不需要计算梯度\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8913d2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # 所有计算得到的tensor的requires_grad自动设置为False\n",
    "        with torch.no_grad():\n",
    "            out = pretrained_model(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "#         print(out.last_hidden_state.shape)  # torch.Size([16, 100, 768])\n",
    "#         print(out.last_hidden_state[:, 0].shape)  # torch.Size([16, 768]) # 注意取0位置\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "        # 经过激活函数\n",
    "        out = out.softmax(dim=1)  \n",
    "        return out\n",
    "\n",
    "# 加载整个模型\n",
    "model = Model()\n",
    "# 同样要移动到cuda\n",
    "model.to(device)\n",
    "\n",
    "# 虚拟一批数据,需要把所有的数据都移动到cuda上\n",
    "input_ids = torch.ones(16, 100).long().to(device) \n",
    "attention_mask = torch.ones(16, 100).long().to(device)\n",
    "token_type_ids = torch.ones(16, 100).long().to(device)\n",
    "labels = torch.ones(16).long().to(device)\n",
    "\n",
    "#试算\n",
    "out = model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids)\n",
    "out\n",
    "out.shape\n",
    "#后面的计算和中文分类完全一样，只是放在了cuda上计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47148449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset chn_senti_corp (C:/Users/lizhong/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9600,\n",
       " ('选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "  1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_dataset(path='seamew/ChnSentiCorp', split=split)\n",
    "#         self.dataset = load_from_disk('./data/ChnSentiCorp')[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "        return text, label\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc28c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset chn_senti_corp (C:/Users/lizhong/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n"
     ]
    }
   ],
   "source": [
    "#加载字典和分词工具，进行编码\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=50,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    " \n",
    "    input_ids = data['input_ids'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    token_type_ids = data['token_type_ids'].to(device)\n",
    "    labels = torch.LongTensor(labels).to(device)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=Dataset(\"train\"),\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "# for i, (input_ids, attention_mask, token_type_ids,\n",
    "#         labels) in enumerate(loader):\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f36fc223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b4486c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\anaconda3\\envs\\py3.9\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "第0批数据： 0.6922425031661987 0.4375\n",
      "第50批数据： 0.509463906288147 0.9375\n",
      "第100批数据： 0.4561876654624939 0.9375\n",
      "第150批数据： 0.457298219203949 0.875\n",
      "第200批数据： 0.3816230893135071 1.0\n",
      "第250批数据： 0.5415152311325073 0.8125\n",
      "第300批数据： 0.417835533618927 0.9375\n",
      "第350批数据： 0.43066173791885376 0.9375\n",
      "第400批数据： 0.6028048992156982 0.6875\n",
      "第450批数据： 0.4416239261627197 0.875\n",
      "第500批数据： 0.43899375200271606 0.875\n",
      "第550批数据： 0.45590609312057495 0.8125\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "# 优化器\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    # 训练\n",
    "    model.train()\n",
    "    \n",
    "    for i, (input_ids, attention_mask, token_type_ids,labels) in enumerate(loader):\n",
    "        # 正向传播\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "       # 计算损失\n",
    "        loss = criterion(out, labels)\n",
    "        # 反向传播，计算梯度\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "            print(f\"第{i}批数据：\", loss.item(), accuracy)\n",
    "\n",
    "        if i == 1000:\n",
    "            break\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"start...\")\n",
    "    train()\n",
    "    print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99968587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset chn_senti_corp (C:/Users/lizhong/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "0.831081081081081\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "        \n",
    "#         print(i)\n",
    "        if i == 1000:\n",
    "            break\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"start...\")\n",
    "    test()\n",
    "    print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7cfba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
